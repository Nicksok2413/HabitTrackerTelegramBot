# Определяем "шаблон" для API-сервисов с помощью якоря
x-api-base: &api-base-config
  build:
    context: .  # Контекст сборки - корень проекта
    dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
    target: api  # Этап №3 'api'
  env_file: .env  # Передаем все переменные из .env в контейнер
  volumes:
    - api_logs:/logs  # Монтируем том для логов
  depends_on:
    db:
      condition: service_healthy  # Запускаем только после того, как db станет healthy
  networks:
    - backend

services:
  # Сервис базы данных (PostgreSQL)
  db:
    image: postgres:18-alpine
    container_name: habit_tracker_db
    environment:
      POSTGRES_DB: ${DB_NAME:-habit_tracker_db}
      POSTGRES_USER: ${DB_USER:-habit_tracker_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data  # Монтируем том для БД
    healthcheck:
      # Проверяет, готова ли БД принимать соединения (экранируем $ для shell)
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s  # Даем время на запуск Postgres перед первой проверкой
    restart: unless-stopped
    networks:
      - backend

  # Redis (брокер сообщений)
  redis:
    image: redis:7-alpine
    container_name: habit_tracker_redis
    # Команда для персистентности (сохранять дамп БД каждые 60 сек, если было хотя бы 1 изменение)
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data  # Монтируем том для сохранения dump.rdb
    healthcheck:
      # Проверяет, отвечает ли Redis на команду PING
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s  # Даем время на запуск Redis перед первой проверкой
    restart: unless-stopped
    networks:
      - backend

  # Сервис API
  api:
    <<: *api-base-config
    container_name: habit_tracker_api
    # Команда для "продакшен" запуска (будет передана в entrypoint как "$@")
    command: python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000
    healthcheck:
      # Проверяет, готов ли api принимать запросы
      test: [ "CMD", "curl", "-f", "http://0.0.0.0:8000/healthcheck" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s  # Даем время на запуск api перед первой проверкой
    restart: unless-stopped

  # Сервис миграций
  api-migrate:
    <<: *api-base-config
    # Сервис запустится только если явно попросить профиль 'tools' или сам сервис
    profiles: ["tools"]
    # Команда для выполнения миграций (будет передана в entrypoint как "$@")
    command: python -m alembic -c pyproject.toml upgrade head
    restart: "no"

  # Сервис телеграм-бота
  bot:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
      target: bot  # Этап №4 'bot'
    container_name: habit_tracker_bot
    # Команда для запуска бота (будет передана в entrypoint как "$@")
    command: python src/bot/main.py
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - bot_logs:/logs  # Монтируем том для логов
    depends_on:
      api:
        condition: service_healthy # Запускаем только после того, как api станет healthy
    restart: unless-stopped
    networks:
      - backend

  # Сервис планировщика
  scheduler:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
      target: scheduler  # Этап №5 'scheduler'
    container_name: habit_tracker_scheduler
    # Команда для запуска планировщика (будет передана в entrypoint как "$@")
    command: python src/scheduler/main.py
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - scheduler_logs:/logs  # Монтируем том для логов
    depends_on:
      db:
        condition: service_healthy  # Запускаем только после того, как db станет healthy
      redis:
        condition: service_healthy  # Ждем Redis, так как туда отправляются задачи
    restart: unless-stopped
    networks:
      - backend

  # Воркер (Celery)
  worker:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
      target: worker  # Этап №6 'worker'
    container_name: habit_tracker_worker
    # Команда для запуска воркера (будет передана в entrypoint как "$@")
    command: celery -A src.worker.celery_app worker -l info -c 2
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - worker_logs:/logs  # Монтируем том для логов
    depends_on:
      db:
        condition: service_healthy  # Запускаем только после того, как db станет healthy
      redis:
        condition: service_healthy  # Запускаем только после того, как redis станет healthy
    restart: unless-stopped
    networks:
      - backend

# Определяем все тома для хранения персистентных данных
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  api_logs:
    driver: local
  bot_logs:
    driver: local
  scheduler_logs:
    driver: local
  worker_logs:
    driver: local

# Определяем общую сеть для всех сервисов.
# Это обеспечивает изоляцию и позволяет контейнерам общаться друг с другом по именам.
networks:
  backend:
    driver: bridge