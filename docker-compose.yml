# Определяем "шаблон" для API-сервисов с помощью якоря
x-api-base: &api-base-config
  build:
    context: .  # Контекст сборки - корень проекта
    dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
    target: api  # Этап №3 'api'
  env_file: .env  # Передаем все переменные из .env в контейнер
  volumes:
    - api_logs:/logs  # Монтируем том для логов
  depends_on:
    db:
      condition: service_healthy  # Запускаем только после того, как db станет healthy
  networks:
    - backend

services:

  # --- APP STACK ---

  # Сервис базы данных (PostgreSQL)
  db:
    image: postgres:18-alpine
    container_name: habit_tracker_db
    environment:
      POSTGRES_DB: ${DB_NAME:-habit_tracker_db}
      POSTGRES_USER: ${DB_USER:-habit_tracker_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data  # Монтируем том для БД
    healthcheck:
      # Проверяет, готова ли БД принимать соединения (экранируем $ для shell)
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s  # Даем время на запуск Postgres перед первой проверкой
    restart: unless-stopped
    networks:
      - backend

  # Redis (брокер сообщений)
  redis:
    image: redis:7-alpine
    container_name: habit_tracker_redis
    # Команда для персистентности (сохранять дамп БД каждые 60 сек, если было хотя бы 1 изменение)
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data  # Монтируем том для сохранения dump.rdb
    healthcheck:
      # Проверяет, отвечает ли Redis на команду PING
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s  # Даем время на запуск Redis перед первой проверкой
    restart: unless-stopped
    networks:
      - backend

  # Сервис API
  api:
    <<: *api-base-config
    container_name: habit_tracker_api
    # Команда для "продакшен" запуска (будет передана в entrypoint как "$@")
    command: python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000
    healthcheck:
      # Проверяет, готов ли api принимать запросы
      test: [ "CMD", "curl", "-f", "http://localhost:8000/healthcheck" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s  # Даем время на запуск api перед первой проверкой
    restart: unless-stopped

  # Сервис миграций
  api-migrate:
    <<: *api-base-config
    # Сервис запустится только если явно попросить профиль 'tools' или сам сервис
    profiles: ["tools"]
    # Команда для выполнения миграций (будет передана в entrypoint как "$@")
    command: python -m alembic -c pyproject.toml upgrade head
    restart: "no"

  # Сервис телеграм-бота
  bot:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
      target: bot  # Этап №4 'bot'
    container_name: habit_tracker_bot
    # Команда для запуска бота (будет передана в entrypoint как "$@")
    command: python src/bot/main.py
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - bot_logs:/logs  # Монтируем том для логов
    depends_on:
      api:
        condition: service_healthy  # Запускаем только после того, как api станет healthy
    restart: unless-stopped
    networks:
      - backend

  # Сервис планировщика
  scheduler:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
      target: scheduler  # Этап №5 'scheduler'
    container_name: habit_tracker_scheduler
    # Команда для запуска планировщика (будет передана в entrypoint как "$@")
    command: python src/scheduler/main.py
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - scheduler_logs:/logs  # Монтируем том для логов
    depends_on:
      db:
        condition: service_healthy  # Запускаем только после того, как db станет healthy
      redis:
        condition: service_healthy  # Ждем Redis, так как туда отправляются задачи
    restart: unless-stopped
    networks:
      - backend

  # Воркер (Celery)
  worker:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/Dockerfile  # Путь к общему Dockerfile
      target: worker  # Этап №6 'worker'
    container_name: habit_tracker_worker
    # Команда для запуска воркера (будет передана в entrypoint как "$@")
    command: celery -A src.worker.celery_app worker -l info -c 2
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - worker_logs:/logs  # Монтируем том для логов
    depends_on:
      db:
        condition: service_healthy  # Запускаем только после того, как db станет healthy
      redis:
        condition: service_healthy  # Запускаем только после того, как redis станет healthy
    restart: unless-stopped
    networks:
      - backend

  # --- MONITORING STACK ---

  # Prometheus (сбор метрик)
  prometheus:
    image: prom/prometheus:latest
    container_name: monitoring_prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro  # Монтируем конфиг (только для чтения)
      - prometheus_data:/prometheus  # Монтируем том для хранения собранных метрик
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'  # Передаем путь к конфигу
      - '--storage.tsdb.retention.time=15d' # Хранить 15 дней
      - '--web.enable-lifecycle'  # Разрешаем перезагрузку конфига "на лету", без необходимости перезапускать контейнер
    depends_on:
      api:
        condition: service_healthy  # Запускаем только после того, как api станет healthy
    restart: unless-stopped
    networks:
      - backend

  # Loki (сбор логов)
  loki:
    image: grafana/loki:3.6.3
    container_name: monitoring_loki
    volumes:
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml:ro  # Монтируем конфиг (только для чтения)
      - loki_data:/loki  # Монтируем том для хранения собранных логов
    command: -config.file=/etc/loki/local-config.yaml  # Передаем путь к конфигу
    restart: unless-stopped
    networks:
      - backend

  # Promtail (агент для отправки логов в Loki)
  promtail:
    image: grafana/promtail:latest
    container_name: monitoring_promtail
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro  # Читаем логи хоста
      - /var/run/docker.sock:/var/run/docker.sock  # Для обнаружения контейнеров
      - ./monitoring/promtail-config.yaml:/etc/promtail/config.yaml:ro  # Монтируем конфиг (только для чтения)
    command: -config.file=/etc/promtail/config.yaml  # Передаем путь к конфигу
    depends_on:
      loki:
        condition: service_started  # Запускаем Promtail после loki
    restart: unless-stopped
    networks:
      - backend

  # Grafana (визуализация)
  grafana:
    image: grafana/grafana:latest
    container_name: monitoring_grafana
    volumes:
      - grafana_data:/var/lib/grafana  # Монтируем том для хранения данных Grafana (настройки, пользователи, дашборды)
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}  # admin - по умолчанию
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}  # admin - по умолчанию
      GF_USERS_ALLOW_SIGN_UP: "false"  # Отключить регистрацию новых пользователей
    ports:
      - "3000:3000"  # Пробрасываем порт для доступа к UI Grafana
    depends_on:
      prometheus:
        condition: service_started  # Запускаем Grafana после prometheus
      loki:
        condition: service_started  # Запускаем Grafana после loki
    restart: unless-stopped
    networks:
      - backend

  # Celery Exporter (метрики фоновых задач)
  # Это "Sidecar" контейнер, который слушает Redis и отдает метрики для Prometheus
  celery-exporter:
    image: danihodovic/celery-exporter:latest
    container_name: monitoring_celery_exporter
    # Передаем URL брокера явно через флаг команды
    command: --broker-url=${REDIS_URL}
    restart: unless-stopped
    networks:
      - backend

# Определяем все тома для хранения персистентных данных
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  api_logs:
    driver: local
  bot_logs:
    driver: local
  scheduler_logs:
    driver: local
  worker_logs:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local

# Определяем общую сеть для всех сервисов
# Это обеспечивает изоляцию и позволяет контейнерам общаться друг с другом по именам
networks:
  backend:
    driver: bridge
